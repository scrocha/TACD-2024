{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b14463-2c4d-4a53-a8d1-def39d052cff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB 07 - Random Forest for Regression\n",
    "\n",
    "In this lab we will be extending the previous lab about Decision trees and build a Regression model using Random Forest.\n",
    "\n",
    "For simplicity, we will be using the same dataset as the previous lab (you can find it in ECLASS).\n",
    "\n",
    "**IMPORTANT:** For this lab, if you haven't finished your code from last week's lab on Decision trees, you will have the option to use the sklearn implementation for a regression tree. However, this doesn't mean that you should skip the previous lab. This is just so that you don't get behind with the content and you don't spend all your time today working on the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3323dc4-95cd-4564-8ccd-441019188fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29dfe0-10af-4871-ab66-e7cffe0685cc",
   "metadata": {},
   "source": [
    "As mentioned before, use the Boston Housing data and prepare your train/val/test split as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fb0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('./BostonHousing.txt')\n",
    "X = data.values[:,:-1]\n",
    "y = data.values[:,-1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2045c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "random_state=42\n",
    "X_train, X_tv, y_train, y_tv = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_tv, y_tv, test_size=0.5, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6c793-4f97-49ff-a1f3-3e5ec2aa086b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1 -- Bootstrap\n",
    "\n",
    "Also known as [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating), this technique consists of making several samples with replacement of the original data, using each of the samples to train an estimator, and then aggregating the predictions using the average (this is also a type of model ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7aaba32-4c00-404c-9899-7a5759059598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X:np.ndarray, num_bags:int=10) -> list:\n",
    "    \"\"\"\n",
    "    Given a dataset and a number of bags,\n",
    "    sample the dataset with replacement.\n",
    "    \n",
    "    This function does not return a copy\n",
    "    of the datapoints, but a list of indices\n",
    "    with compatible dimensionality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        A dataset\n",
    "    num_bags : int, default 10\n",
    "        The number of bags to create\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of ndarray\n",
    "        The list contains `num_bags` integer one-dimensional ndarrays.\n",
    "        Each of these contains the indices corresponding to the \n",
    "        sampled datapoints in `X`\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    * The number of datapoints in each bach will\n",
    "      match the number of datapoints in the given\n",
    "      dataset.\n",
    "    * The\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0) # you can change the seed, or use 0 to replicate my results\n",
    "    # Your code here\n",
    "    samples = len(X)\n",
    "    bags = list()\n",
    "\n",
    "    for bag in range(num_bags):\n",
    "        indices = rng.choice(samples, size=samples, replace=True)\n",
    "        bags.append(indices)\n",
    "\n",
    "    return bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff32d32b-f615-43da-b70e-aa8959d01093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([85, 63, 51, 26, 30,  4,  7,  1, 17, 81, 64, 91, 50, 60, 97, 72, 63,\n",
       "       54, 55, 93, 27, 81, 67,  0, 39, 85, 55,  3, 76, 72, 84, 17,  8, 86,\n",
       "        2, 54,  8, 29, 48, 42, 40,  2,  0, 12,  0, 67, 52, 64, 25, 61, 76,\n",
       "       38, 46, 99, 80, 98, 37, 68, 95, 65, 84, 68, 70, 38, 87, 13, 57, 72,\n",
       "       84, 52, 37, 31, 42, 48, 71, 88,  7, 93, 53, 35, 67, 57, 25, 32, 71,\n",
       "       59, 50, 33, 76, 39, 32, 89, 26, 22, 71, 62,  4,  8, 37, 83],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X_small = rng.random(size=(100,2))\n",
    "bags = bootstrap(X_small)\n",
    "bags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b5126-5aad-4c74-a7de-ad3935acd7e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2 -- Aggregation\n",
    "\n",
    "The second part of bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baac85e0-2a36-4ee0-92a5-e31a01a70928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_regression(preds:list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Aggregate predictions by several estimators\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : list of ndarray\n",
    "        Predictions from multiple estimators.\n",
    "        All ndarrays in this list should have the same\n",
    "        dimensionality.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    ndarray\n",
    "        The mean of the predictions\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    sum_preds = np.zeros_like(preds[0])\n",
    "\n",
    "    for pred in preds:\n",
    "        sum_preds += pred\n",
    "    \n",
    "    return sum_preds/len(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91f57a-6395-4285-ac01-e1099af84dde",
   "metadata": {},
   "source": [
    "## Exercise 3 -- Random Forest for regression\n",
    "\n",
    "Using the functions you implemented above, it is now time to put all of them together to train several decision trees and then ensemble them to output a single prediction. For the random forest, however, we need to select a subset of features at each split on the decision tree. \n",
    "\n",
    "For this part, you can use the sklearn implementation of Random forest for regression as your estimator for each set of features and bags. See below an example of how to do this, and always remember to check the necessary documentation when using an external function.\n",
    "\n",
    "Some parameters you will have to set are: \n",
    "* num_features: number of features per estimator\n",
    "* min_samples: min number of samples per leaf node\n",
    "* max_depth: maximum depth of the decision tree (each estimator)\n",
    "* num_estimators: number of decision trees you will create using each bag and random set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71136bc0-c316-41d9-8832-c2db217101ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.43181818, 21.63205128, 27.49767442, 21.63205128, 44.15789474,\n",
       "       21.22      , 21.63205128, 14.77142857, 16.43181818, 21.63205128,\n",
       "       44.15789474, 21.63205128, 10.27567568, 27.49767442, 21.63205128,\n",
       "       16.43181818, 16.43181818, 21.63205128, 10.27567568, 21.63205128,\n",
       "       21.63205128, 27.49767442, 16.43181818, 27.49767442, 21.63205128,\n",
       "       14.77142857, 32.63243243, 21.63205128, 21.63205128, 21.63205128,\n",
       "       21.63205128, 16.43181818, 21.63205128, 21.63205128, 16.43181818,\n",
       "       27.49767442, 21.63205128, 10.27567568, 16.43181818, 16.43181818,\n",
       "       16.43181818, 10.27567568, 16.43181818, 16.43181818, 21.63205128,\n",
       "       10.27567568, 14.77142857, 14.77142857, 10.27567568, 14.77142857,\n",
       "       44.15789474])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of sklearn Decision tree\n",
    "estimator = DecisionTreeRegressor(max_depth=4)\n",
    "estimator.fit(X_train, y_train)\n",
    "estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48ecbd9d-9628-42d4-88ee-c3c3ea408e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Val Set: 10.689876060537863\n"
     ]
    }
   ],
   "source": [
    "def train_random_forest(X:np.ndarray, y:np.ndarray, num_features:int, min_samples:int, max_depth:int, num_estimators:int) -> list:\n",
    "    forest = list()\n",
    "    bags = bootstrap(X, num_estimators)\n",
    "\n",
    "    for indices in bags:\n",
    "        X_sample = X[indices]\n",
    "        y_sample = y[indices]\n",
    "\n",
    "        features =  np.random.choice(X.shape[1], num_features, replace=False)\n",
    "        X_sample = X_sample[:, features]\n",
    "\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples)\n",
    "        tree.fit(X_sample, y_sample)\n",
    "\n",
    "        forest.append([tree, features])\n",
    "\n",
    "    return forest\n",
    "\n",
    "def predict_random_forest(X:np.ndarray, forest:list) -> np.ndarray:\n",
    "    predictions = []\n",
    "    \n",
    "    for tree, features in forest:\n",
    "        pred = tree.predict(X[:, features])\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    return aggregate_regression(predictions)\n",
    "\n",
    "def rmse(y_real:np.ndarray, y_pred:np.ndarray) -> float:\n",
    "    return np.sqrt(np.mean(np.power(y_real - y_pred, 2)))\n",
    "\n",
    "num_features = 3\n",
    "min_samples = 25\n",
    "max_depth = 10\n",
    "num_estimators = 100\n",
    "\n",
    "forest = train_random_forest(X_train, y_train, num_features, min_samples, max_depth, num_estimators)\n",
    "\n",
    "y_val_pred = predict_random_forest(X_val, forest)\n",
    "print(f\"RMSE Val Set: {rmse(y_val, y_val_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
