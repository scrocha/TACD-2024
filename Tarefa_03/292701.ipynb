{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64a3066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292701"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "292701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0884a9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d1afd-bbf2-48d1-93be-79fbbdd43547",
   "metadata": {},
   "source": [
    "# Tarefa 3 - Neural Networks\n",
    "Third assessed coursework for the course: Técnicas e Algoritmos em Ciência de Dados\n",
    "\n",
    "This tarefa provides an exciting opportunity for students to put their knowledge acquired in class into practice, using neural networks to solve real-world problems in both classification and regression. Students will apply the concepts they have learned to build, train, and optimize neural networks, using a validation set to fine-tune hyperparameters. Students will also get used to generating important plots during training to analyse the models' behaviour. By the end of the project, students will have gained hands-on experience in implementing neural networks.\n",
    "\n",
    "## General guidelines:\n",
    "\n",
    "* This work must be entirely original. You are allowed to research documentation for specific libraries, but copying solutions from the internet or your classmates is strictly prohibited. Any such actions will result in a deduction of points for the coursework.\n",
    "* Please enter your code in the designated areas of the notebook. You can create additional code cells to experiment with, but __make sure to place your final solutions where they are requested in the notebook.__\n",
    "* Before submitting your work, make sure to rename the file to the random number that you created for the previous coursework (for example, 289479.ipynb).\n",
    "\n",
    "## Notebook Overview:\n",
    "\n",
    "1. [Regression](#Regression) (50%)\n",
    "2. [Classification](#Classification) (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd2f07-f367-4318-96f4-56d85d13c8f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regression\n",
    "\n",
    "**Download from ECLASS**\n",
    "- Tarefa_3_template.ipynb\n",
    "- energy_efficiency.csv\n",
    "\n",
    "**Dataset and Problem Description**\n",
    "\n",
    "In this exercise, you will use the Energy Efficiency Prediction dataset. This dataset contains information about the energy efficiency of buildings based on eight features, including the size of the building, the orientation, and the type of building materials used. The dataset includes two targets: heating load and cooling load, which represent the energy required to heat and cool the building, respectively.\n",
    "\n",
    "This dataset is useful for building neural networks that predict the energy efficiency of buildings, which is an important problem in the field of sustainable energy. The dataset has been used in several machine learning research papers and provides a challenging regression problem.\n",
    "\n",
    "**Exercise Description: Energy Efficiency Prediction with Neural Networks**\n",
    "\n",
    "In this exercise, you will use the Energy Efficiency Prediction dataset provided.\n",
    "You will build and train a neural network to predict the heating load (column labelled y1 in the dataset) and the cooling load (column labelled y2) of a building based on its energy efficiency features. \n",
    "\n",
    "\n",
    "### To complete this exercise, you will write code to build and train neural networks for this problem:\n",
    "\n",
    "1. Split the dataset into training, validation, and test sets, using a 70:15:15 ratio.\n",
    "\n",
    "2. Use numpy, build a neural network that takes the energy efficiency features as input and predicts the heating and the cooling load as outputs. You will choose the number of neurons per layers and the number of layers, but each layer will have the same number of neurons. These two values must be input parameters for your neural network. That is, you can’t hard-code each layer, meaning that you will have to write code that is able to work with a variable number of layers and neurons. \n",
    "\n",
    "3. Code the forward pass and backpropagation algorithm to learn the weights of the neural network. Use the training set to train the neural network and update the weights using stochastic gradient descent. For the hidden layers use the sigmoid activation function. You will need to regularize your neural network using weight decay, that is, you will include a regularization term in your error function.\n",
    "\n",
    "4. Monitor the training by plotting the training and validation losses across the epochs. \n",
    "\n",
    "The performance of your neural network will be different depending on the number of layers, number of neurons per layer and the value of λ that controls the amount of weight decay. You will experiment with 3 values of λ: 0 (no weight decay), 0.001 and 0.0001.\n",
    "To choose the best network configuration and assess its performance you will:\n",
    "\n",
    "1. Choose 3 possible values of number hidden layers (for example, 1 to 3) and 3 different values of neurons per layer (for example, 100, 200, and 300), but you can also choose different values. \n",
    "\n",
    "2. Calculate the loss for each configuration on the validation set.\n",
    "\n",
    "3. Generate 3 heatmaps, one for each value of the λ regularization parameter, displaying the loss on the validation set by plotting the number of layers and number of neurons in a grid. This will help you visualise the best configuration for the neural network. \n",
    "\n",
    "4. Train your final model selecting the best combination of hyper-parameters and evaluate the final performance of the neural network using the test set and the root mean squared error as the metric and report that.\n",
    "\n",
    "**Important:**\n",
    "- Train for 50 epochs.\n",
    "- Set the learning rate η to 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a97e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading csv and separating between data and target\n",
    "random_state = 49\n",
    "data = pd.read_csv(\"./energy_efficiency.csv\")\n",
    "X = data.values[:,:-2]\n",
    "y = data.values[:,-2:]\n",
    "# Spliting the data\n",
    "X_train, X_tv, y_train, y_tv = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_tv, y_tv, test_size=0.5, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "186fb6bf-419a-4895-b9df-b31b5278b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code goes here:\n",
    "def rmse(y_pred:np.ndarray, y_real:np.ndarray) -> float:\n",
    "    return np.sqrt(np.mean(np.power(y_pred - y_real, 2)))\n",
    "\n",
    "def sse(y_pred:np.ndarray, y_real:np.ndarray) -> float:\n",
    "    return np.sum(np.power(y_pred - y_real, 2))\n",
    "\n",
    "def sigmoid_activation(z:np.ndarray) -> np.ndarray:\n",
    "    z = z.astype(float)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z:np.ndarray) -> np.ndarray:\n",
    "    return sigmoid_activation(z) * (1 - sigmoid_activation(z))\n",
    "\n",
    "def gen_weights_bias(nlines: int, ncols: int, std:float=0.01) -> list[np.ndarray]:\n",
    "    return [np.random.normal(size=(nlines, ncols), scale=std), np.random.normal(size=(ncols, 1), scale=std)]\n",
    "\n",
    "def get_rmse_model(X: np.ndarray, y_real:np.ndarray, model:dict, activation=sigmoid_activation) -> float:\n",
    "    X = X @ model[0][0] * model[0][1].T\n",
    "\n",
    "    for layer in list(model.keys())[1:]:\n",
    "        X = activation(X) @ model[layer][0] * model[layer][1].T\n",
    "\n",
    "    y_pred = X\n",
    "    return rmse(y_pred=y_pred, y_real=y_real)\n",
    "\n",
    "def neural_network_train(X:np.ndarray, y_real:np.ndarray, neurons_amount:int,\n",
    "                         layers_num:int, epochs:int,\n",
    "                         weight_decay:float, learning_rate:float) -> list[list[np.ndarray]]:\n",
    "    # Initialize the layers with random values and save in a list of layers\n",
    "    model_dict = dict()\n",
    "\n",
    "\n",
    "    if layers_num == 1:\n",
    "        model_dict[0] = gen_weights_bias(X.shape[1], y_real.shape[1])\n",
    "    else:\n",
    "        model_dict[0] = gen_weights_bias(X.shape[1], neurons_amount)\n",
    "\n",
    "        if layers_num > 2:\n",
    "            for i in range(1, layers_num-1):\n",
    "                model_dict[i] = gen_weights_bias(neurons_amount, neurons_amount)\n",
    "                \n",
    "        model_dict[layers_num-1] = gen_weights_bias(neurons_amount, y_real.shape[1])\n",
    "\n",
    "    sse_historic = list()\n",
    "    # For each epoch we ran our train again\n",
    "    for epoch in range(epochs):\n",
    "        # Permutate the indices to exclude a vies\n",
    "        indices = np.random.permutation(len(X))\n",
    "        X_permuted = X[indices]\n",
    "        y_permuted = y_real[indices]\n",
    "\n",
    "        # Start the Stochastic Gradient Descendent method\n",
    "        for i in range(len(X)):\n",
    "            # Get a sample of X and y permuted\n",
    "            x_sample = X_permuted[i]\n",
    "            y_sample = y_permuted[i]\n",
    "            \n",
    "            # X sample is a line vector\n",
    "            x_sample = x_sample.reshape(1, -1)\n",
    "            y_sample = y_sample.reshape(1, -1)\n",
    "            \n",
    "            # Start the layers activations by doing\n",
    "            layers = dict()\n",
    "            \n",
    "            layers[0] = x_sample @ model_dict[0][0] + model_dict[0][1].T\n",
    "            \n",
    "            for i in range(1, layers_num):\n",
    "                layers[i] = sigmoid_activation(layers[i-1]) @ model_dict[i][0] + model_dict[i][1].T\n",
    "                \n",
    "            error_current = layers[layers_num-1] - y_sample # e_o = a_o - y\n",
    "\n",
    "            for i in range(layers_num-1, 0, -1):\n",
    "\n",
    "                derivative_w = sigmoid_activation(layers[i-1]).T @ error_current # DW_o = a_o-1.T @ e_o\n",
    "                derivative_b = np.sum(error_current, axis=0).reshape(-1, 1) # Db_o = e_o\n",
    "                \n",
    "                model_dict[i][0] -= learning_rate * (derivative_w + weight_decay * model_dict[i][0])\n",
    "                model_dict[i][1] -= learning_rate * derivative_b\n",
    "\n",
    "                error_current = (error_current @ model_dict[i][0].T) * sigmoid_derivative(layers[i-1])\n",
    "\n",
    "            derivative_wi = x_sample.T @ error_current # DW_o = a_o-1.T @ e_o\n",
    "            derivative_bi = np.sum(error_current, axis=0).reshape(-1, 1) # Db_o = e_o\n",
    "                \n",
    "            model_dict[0][0] -= learning_rate * (derivative_wi + weight_decay * model_dict[0][0])\n",
    "            model_dict[0][1] -= learning_rate * derivative_bi\n",
    "\n",
    "        sse_current = get_rmse_model(X, y_real, model_dict)\n",
    "        sse_historic.append(sse_current)\n",
    "\n",
    "    return model_dict, sse_historic\n",
    "\n",
    "# neural_network_train(X_train, y_train, 50, 1, 100, 0.001, 0.001)[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2909b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here:\n",
    "# your code goes here\n",
    "\n",
    "def backpropagation_2layer_wd(X:np.ndarray, y_real:np.ndarray, dim_input:int,\n",
    "                            dim_hidden:int, dim_output:int, lr:float=0.0001, wd:float=0.001, repeats:int=300) -> tuple[np.ndarray]:\n",
    "    \n",
    "    # X  (n, m)\n",
    "    # Initializing weights and biases with random values\n",
    "    # First we initialize the input to hidden layers of weights and biases\n",
    "    W_input = np.random.normal(size=(dim_input, dim_hidden)) # Wi (m, h)\n",
    "    b_input = np.random.normal(size=(dim_hidden, 1)) # bi (h, 1)\n",
    "    # Then the hidden to output layers of weights and biases\n",
    "    W_output = np.random.normal(size=(dim_hidden, dim_output)) # Wo (h, o)\n",
    "    b_output = np.random.normal(size=(dim_output, 1)) # bo (o, 1)\n",
    "\n",
    "    \n",
    "    for repeat in range(repeats):\n",
    "        hidden_layer_i = X @ W_input + b_input.T # (n, m) x (m, h) -> (n, h)\n",
    "        hidden_layer_o = sigmoid_activation(hidden_layer_i)\n",
    "\n",
    "        output_layer = hidden_layer_o @ W_output + b_output.T # (n, h) x (h, o) -> (n, o)\n",
    "        error_o = output_layer - y_real # (n, o)\n",
    "\n",
    "        derivative_wo = hidden_layer_o.T @ error_o # (h, n) x (n, o) -> (h, o)\n",
    "        derivative_bo = np.sum(error_o, axis=0).reshape(-1, 1) # (o, 1)\n",
    "\n",
    "        error_h = error_o @ W_output.T * sigmoid_derivative(hidden_layer_i) # (n, o) x (o, h) -> (n, h) * (n, h)\n",
    "    \n",
    "        derivative_wi = X.T @ error_h # (m, n) x (n, h)\n",
    "        derivative_bi = np.sum(error_h, axis=0).reshape(-1, 1) # (h, 1)\n",
    "\n",
    "        derivative_wo += wd * W_output\n",
    "        derivative_wi += wd * W_input\n",
    "\n",
    "        W_output -= lr * derivative_wo\n",
    "        b_output -= lr * derivative_bo\n",
    "    \n",
    "        W_input -= lr * derivative_wi\n",
    "        b_input -= lr * derivative_bi\n",
    "\n",
    "    return W_input, b_input, W_output, b_output\n",
    "\n",
    "\n",
    "# for i in [50, 100, 200]:\n",
    "#     hidden_layer_dim = i\n",
    "\n",
    "#     W_input, b_input, W_output, b_output = backpropagation_2layer_wd(X_train, y_train, X_train.shape[1], hidden_layer_dim, y_train.shape[1])\n",
    "    \n",
    "#     y_val_pred = sigmoid_activation(X_val @ W_input + b_input.T) @ W_output + b_output.T\n",
    "#     val_rmse = rmse(y_val_pred, y_val)\n",
    "#     print(f\"RMSE para {i} neurônio(s): {val_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de5467-3e3c-45d4-bf03-04baeaf20193",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "**Download the data from ECLASS**\n",
    "- drug_side_effects.csv\n",
    "- drug_features.csv\n",
    "\n",
    "**Dataset description:**\n",
    "\n",
    "In this exercise, you will build and train a neural network to predict the occurrence of drug side effects. The dataset is derived from the SIDER dataset, containing relatively common side effects that can occur for at least 50 drugs. This produces a total of 740 drugs and 256 side effects. The features represent various molecular properties, including molecular weight, number of atoms, number of rings, number of hydrogen bond donors and acceptors, logP, topological polar surface area (TPSA), number of rotatable bonds, number of aromatic rings, number of aliphatic rings, number of saturated rings, and number of heteroatoms. \n",
    "\n",
    "**Remember that each drug can cause many side effects, not only one.** \n",
    "\n",
    "*Feel free to explore the dataset and check the potential side effects of popular medications!*\n",
    "\n",
    "### To complete this exercise, follow these steps:\n",
    "\n",
    "1. Load the dataset and split it into training, validation, and test sets, using an 80:10:10 ratio. \n",
    "\n",
    "2. Standardize the features by removing the mean and scaling to unit variance. To do this, perform the following for each feature (column) in the dataset:\n",
    "    - Calculate the mean and standard deviation across the training set for that feature.\n",
    "    - Subtract the mean from each value in that feature and divide by the standard deviation.\n",
    "    - Apply the same transformation to the validation and test sets using the mean and standard deviation calculated from the training set.\n",
    "\n",
    "**Observation:** you need to code this part, you’re not allowed to use scikit-learn.\n",
    "\n",
    "*Normalization of features is important for neural networks because:*\n",
    "- *It ensures that all features have the same scale, preventing certain features from dominating the learning process due to their larger magnitude.*\n",
    "- *It improves the numerical stability of the training process, making the neural network less sensitive to the choice of learning rate and other hyperparameters.*\n",
    "\n",
    "3. Build a neural network using NumPy that takes in the features as input and predicts the occurrence of side effects. You will choose the number of neurons per layer and the number of layers. You will provide this information as an input list where the length of the list determines the number of hidden layers, and each element is the number of neurons of that hidden layer. For example, an array `layers = [64,128,256]` should produce a network with 4 layers, with 3 hidden layers with 64, 128, and 256 neurons each. For the hidden layers use the sigmoid activation function. You will need to regularize your neural network using weight decay, that is, you will include a regularization term in your error function.\n",
    "\n",
    "4. Code the forward pass and backpropagation algorithm to learn the weights of the neural network. Use the training set to train the neural network and update the weights using stochastic gradient descent. Don’t forget about the biases. \n",
    "\n",
    "5. Monitor the training by plotting the training and validation losses across the epochs.\n",
    "\n",
    "\t**Observation:** make sure the loss goes down during training, acceptable values are within 0.2 – 2.8 approximately. These values depend on the choice of the different hyperparameters. Test only sensible values taking into account the dataset, i.e., number of features, drugs, side effects. \n",
    "\n",
    "The performance of your neural network will be different depending on the number of layers, number of neurons per layer and the value of λ that controls the amount of weight decay. You will experiment with 3 values of λ: 0 (no weight decay), 1 and 0.01.\n",
    "To choose the best network configuration and assess its performance you will:\n",
    "\n",
    "1. For each value of λ, select 3 different layer configurations (note that in this exercise, the number of neurons per layer does not require to be the same for each layer).\n",
    "2. Calculate the loss for each configuration on the validation set.\n",
    "3. At the end of this process, you should be left with 9 loss values (one for each configuration). Train your final model selecting the best combination of hyper-parameters and evaluate the final performance of the neural network using the test set and the Area Under the ROC Curve (AUROC) with the function provided in the Jupyter notebook. \n",
    "\t\n",
    "\t*Observation: don’t expect impressive AUROC values, as this is a highly complex problem that can’t be solved easily with a simple neural network with standard features. Expect values in the range (0.55-0.75).*\n",
    "\n",
    "**Important:**\n",
    "- Train for 50 epochs.\n",
    "- Set the learning rate η to 0.01.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad0c6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_side_effects = pd.read_csv(\"drug_side_effects.csv\")\n",
    "drug_features = pd.read_csv(\"drug_features.csv\")\n",
    "\n",
    "X = drug_features.values[:,1:]\n",
    "y = drug_side_effects.values[:,1:]\n",
    "\n",
    "X_train, X_tv, y_train, y_tv = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_tv, y_tv, test_size=0.5, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1256e9d5-779c-4a34-96ca-44275e9efc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## your code goes here:\n",
    "def normalize_data(data:np.ndarray) -> np.ndarray:\n",
    "    for col in range(data.shape[1]):\n",
    "        data[:, col] = (data[:, col] - np.mean(data[:, col]))/np.std(data[:, col])\n",
    "    return data\n",
    "\n",
    "def normalize_all_data(*args:list[np.ndarray]) -> list[np.ndarray]:\n",
    "    normilizeds = list()\n",
    "    for arg in args:\n",
    "        normilizeds.append(normalize_data(arg))\n",
    "    return normilizeds\n",
    "\n",
    "def gen_weights_bias(nlines: int, ncols: int, std:float=0.01) -> list:\n",
    "    return [np.random.normal(size=(nlines, ncols), scale=std), np.random.normal(size=(ncols, 1), scale=std)]\n",
    "\n",
    "def forward_pass(X:np.ndarray, network:dict, activation=sigmoid_activation) -> dict:\n",
    "    layers = dict()\n",
    "    \n",
    "    layers[0] = X @ network[0][0] + network[0][1].T\n",
    "    \n",
    "    for i in range(1, len(network.keys())):\n",
    "        layers[i] = activation(layers[i-1]) @ network[i][0] + network[i][1].T\n",
    "\n",
    "    return layers\n",
    "\n",
    "def backpropagation(network:dict, layers:dict, activation=sigmoid_activation, Dactivation=sigmoid_derivative) -> dict:\n",
    "\n",
    "    return network\n",
    "\n",
    "def classification_network(X:np.ndarray, y_real:np.ndarray, layers:list,\n",
    "                           weight_decay:float, epochs:int=50, lr:float=0.01) -> dict:\n",
    "    network = dict()\n",
    "    \n",
    "    network[0] = gen_weights_bias(X.shape[1], layers[0])\n",
    "    if len(layers) > 1:\n",
    "        for i in range(1, len(layers)):\n",
    "            network[i] = gen_weights_bias(layers[i-1], layers[i])\n",
    "\n",
    "        network[len(layers)] = gen_weights_bias(layers[-1], y_real.shape[1])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Permutate the indices to exclude a vies\n",
    "        indices = np.random.permutation(len(X))\n",
    "        X_permuted = X[indices]\n",
    "        y_permuted = y_real[indices]\n",
    "\n",
    "        # Start the Stochastic Gradient Descendent method\n",
    "        for i in range(len(X)):\n",
    "            # Make x and y samples as a line vector\n",
    "            x_sample = X_permuted[i].reshape(1, -1)\n",
    "            y_sample = y_permuted[i].reshape(1, -1)\n",
    "            \n",
    "            # x_sample = X\n",
    "            # y_sample = y_real\n",
    "            \n",
    "            layers_actived = forward_pass(x_sample, network)\n",
    "\n",
    "            network = backpropagation(network, layers_actived)\n",
    "\n",
    "            break\n",
    "        break\n",
    "\n",
    "    # return network\n",
    "\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = normalize_all_data(X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "\n",
    "classification_network(X_train, y_train, [16, 32, 64], 0, 50, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e0dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to calculate the test set AUROC use the following code:\n",
    "\n",
    "# auroc = roc_auc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c443a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
